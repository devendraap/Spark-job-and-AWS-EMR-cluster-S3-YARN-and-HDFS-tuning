<!DOCTYPE html>
<!-- saved from url=(0080)https://wiki.advisory.com/plugins/viewsource/viewpagesrc.action?pageId=217422325 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>View Source</title>
        <link rel="canonical" href="https://wiki.advisory.com/pages/viewpage.action?pageId=$action.page.id">
        <script>
window.WRM=window.WRM||{};window.WRM._unparsedData=window.WRM._unparsedData||{};window.WRM._unparsedErrors=window.WRM._unparsedErrors||{};
WRM._unparsedData["com.atlassian.plugins.atlassian-plugins-webresource-plugin:context-path.context-path"]="\u0022\u0022";
if(window.WRM._dataArrived)window.WRM._dataArrived();</script>
<link type="text/css" rel="stylesheet" href="./View Source_files/batch.css" data-wrm-key="_super" data-wrm-batch-type="context" media="all">
<!--[if lt IE 9]>
<link type="text/css" rel="stylesheet" href="/s/6efd7cfc522d7ad0138f833a4c89d39e-CDN/en_US/7901/317cab7b29357621c9029e086de05b05c96603dd/648db4f579a59a8a6df133862b99535c/_/download/contextbatch/css/_super/batch.css?conditionalComment=lt+IE+9" data-wrm-key="_super" data-wrm-batch-type="context" media="all">
<![endif]-->
<!--[if lte IE 9]>
<link type="text/css" rel="stylesheet" href="/s/6efd7cfc522d7ad0138f833a4c89d39e-CDN/en_US/7901/317cab7b29357621c9029e086de05b05c96603dd/648db4f579a59a8a6df133862b99535c/_/download/contextbatch/css/_super/batch.css?conditionalComment=lte+IE+9" data-wrm-key="_super" data-wrm-batch-type="context" media="all">
<![endif]-->
<link type="text/css" rel="stylesheet" href="./View Source_files/batch(1).css" data-wrm-key="plugin.viewsource,-_super" data-wrm-batch-type="context" media="all">
<link type="text/css" rel="stylesheet" href="./View Source_files/batch(2).css" data-wrm-key="page,-_super" data-wrm-batch-type="context" media="all">
<link type="text/css" rel="stylesheet" href="./View Source_files/batch(3).css" data-wrm-key="editor-content,-_super" data-wrm-batch-type="context" media="all">
<!--[if lte IE 9]>
<link type="text/css" rel="stylesheet" href="/s/ce16f0a95862a894032cc132bb263c05-CDN/en_US/7901/317cab7b29357621c9029e086de05b05c96603dd/b64582587d2ef514827ddfff76457315/_/download/contextbatch/css/editor-content,-_super/batch.css?conditionalComment=lte+IE+9&amp;confluence.table.resizable=true" data-wrm-key="editor-content,-_super" data-wrm-batch-type="context" media="all">
<![endif]-->
<link type="text/css" rel="stylesheet" href="./View Source_files/colors.css" media="all">
<link type="text/css" rel="stylesheet" href="./View Source_files/custom.css" media="all">

    </head>

    <body class="mceContentBody aui-theme-default wiki-content fullsize">
        <p>&nbsp;</p>         <p>LDE (Longitudinal Data Engineer) team handles all operation related to longitudinal claims data received from various vendors. We process our data in AWS environment using Spark (on EMR) and use object storage (S3) for storage.</p><p>While processing billions of records or TB's of data we faced multiple hurdles.&nbsp;</p><p>This wiki documents extensively the problems LDE team faced and their solutions. The Spark job and cluster optimization for processing large dataset are also documented.</p><p><strong><em>Note: Please go through reference links provided to fully understand how spark options affects to spark data processing</em></strong></p><p><img class="editor-inline-macro" src="./View Source_files/macro" data-macro-name="toc" data-macro-id="5da178d0-8620-424c-b136-be251bf612b2" data-macro-schema-version="1"></p><h2><strong style="font-size: 20.0px;letter-spacing: -0.008em;">Spark job tuning:</strong></h2><h3 style="margin-left: 30.0px;"><strong style="letter-spacing: 0.0px;">Specs per CORE or TASK node of r4.12xlarge instance type:</strong></h3><table class="relative-table confluenceTable" style="margin-left: 60.0px;width: 24.0445%;"><colgroup><col style="width: 22.4928%;"><col style="width: 77.2174%;"></colgroup><tbody style="margin-left: 60.0px;"><tr style="margin-left: 60.0px;"><th style="margin-left: 60.0px;" class="confluenceTh"><p style="text-align: center;">Cores</p></th><td style="margin-left: 60.0px;text-align: center;" class="confluenceTd"><p style="margin-left: 30.0px;">48</p></td></tr><tr style="margin-left: 60.0px;"><th style="margin-left: 60.0px;text-align: center;" class="confluenceTh"><p>Memory</p></th><td style="margin-left: 60.0px;" class="confluenceTd"><p style="margin-left: 30.0px;text-align: center;">(384 GiB&nbsp; * 1000) / 1024 = 375 GB</p></td></tr></tbody></table><h3 style="margin-left: 30.0px;"><strong>Executor resource calculation:</strong></h3><p style="margin-left: 60.0px;">By assigning 1 core and 1GB&nbsp; for YARN, we are left with 47 core per node.</p><p style="margin-left: 60.0px;">We allocated 5 cores per executor for max HDFS throughput</p><p style="margin-left: 60.0px;">MemoryStore&nbsp;and BlockManagerMaster&nbsp;per node consumes 12GB per node</p><p style="margin-left: 60.0px;"><em><span style="letter-spacing: 0.0px;"><br></span></em></p><p style="margin-left: 60.0px;"><em><span style="letter-spacing: 0.0px;">Memory per executor = (374 - 12 -12) / 9 ~= 40 GB</span></em></p><p style="margin-left: 60.0px;"><em>Number of executor&nbsp;= (48 - 1) / 5 ~= 9&nbsp;</em></p><h3 style="margin-left: 30.0px;"><span style="letter-spacing: 0.0px;"><strong style="letter-spacing: 0.0px;">Spark submit options:</strong></span></h3><table class="relative-table confluenceTable" style="width: 100.0%;"><colgroup><col style="width: 23.7665%;"> <col style="width: 15.4322%;"> <col style="width: 19.7825%;"> <col style="width: 20.8357%;"> <col style="width: 20.1946%;"> </colgroup><tbody><tr><th class="confluenceTh">Parameter</th><th class="confluenceTh">Value</th><th class="confluenceTh">Explanation</th><th class="confluenceTh">Benefits</th><th class="confluenceTh">Reference</th></tr><tr><td class="confluenceTd">spark.executor.memory</td><td class="confluenceTd">33g</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td><td rowspan="4" class="confluenceTd"><a class="confluence-link" href="https://wiki.advisory.com/display/~ParhateD/Spark+job+and+cluster+tuning" data-linked-resource-default-alias="Spark job and cluster tuning" data-base-url="https://wiki.advisory.com">Spark executor memory</a></td></tr><tr><td class="confluenceTd">spark.executor.cores</td><td class="confluenceTd">5</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.memory.fraction</td><td class="confluenceTd">0.8</td><td class="confluenceTd">Approx. (spark.memory.fraction * spark.executor.memory) memory for task execution, shuffle, join, sort, aggregate&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.memory.storageFraction</td><td class="confluenceTd">0.5</td><td class="confluenceTd">Approx. (spark.memory.storageFraction * spark.executor.memory) memory for cache, broadcast and accumulator</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.executor.extraJavaOptions</td><td class="confluenceTd">-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:OnOutOfMemoryError='kill -9 %p'</td><td class="confluenceTd">The parameter -XX:+UseG1GC specifies that the G1GC garbage collector should be used. (The default is -XX:+UseParallelGC.) To understand the frequency and execution time of the garbage collection, use the parameters -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps. To initiate garbage collection sooner, set InitiatingHeapOccupancyPercent to 35 (the default is 0.45). Doing this helps avoid potential garbage collection for the total memory, which can take a significant amount of time.</td><td class="confluenceTd">Better garbage collection as G1 is suItable for large heap to resolve Out of memory issue, reduce the gc pause time, high latency and low throughput</td><td class="confluenceTd"><a href="https://spark.apache.org/docs/latest/tuning.html">SPARK official docs</a></td></tr><tr><td class="confluenceTd">spark.driver.maxResultSize</td><td class="confluenceTd">20G</td><td class="confluenceTd">spark.sql.autoBroadcastJoinThreshold &lt; spark.driver.maxResultSize &lt; spark.driver.memory</td><td class="confluenceTd">Resolves error: serialized results of x tasks is bigger than spark.driver.maxResultSize</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.yarn.maxAppAttempts</td><td class="confluenceTd">2</td><td class="confluenceTd">Maximum attempts for running application</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.rpc.message.maxSize</td><td class="confluenceTd">2048</td><td class="confluenceTd">Increases remote procedure call message size</td><td class="confluenceTd">Resolves error: exceeds max allowed: spark.rpc.message.maxSize</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.spark.worker.timeout</td><td class="confluenceTd">240</td><td rowspan="2" class="confluenceTd">Allows task working on skewed data more time for execution. Proper re-partitioning (with salting) on join or groupBy column reduces time for execution</td><td rowspan="2" class="confluenceTd">Lost executor xx on slave1.cluster: Executor heartbeat timed out after xxxxx ms</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.network.timeout</td><td class="confluenceTd">9999s</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.shuffle.file.buffer</td><td class="confluenceTd">1024k</td><td class="confluenceTd">Reduce the number of times the disk file overflows during the shuffle write process, which can reduce the number of disk IO times and improve performance</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.locality.wait</td><td class="confluenceTd">15s</td><td class="confluenceTd">Reduces large amounts of data transfer over network (shuffling)</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.shuffle.io.connectionTimeout</td><td class="confluenceTd">3000</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">Resolves error: “org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]”</td><td class="confluenceTd"><a href="https://developer.ibm.com/hadoop/2016/07/18/troubleshooting-and-tuning-spark-for-heavy-workloads/">IBM: Spark heavy workloads tuning</a></td></tr><tr><td class="confluenceTd">spark.shuffle.io.retryWait</td><td class="confluenceTd">60s</td><td class="confluenceTd">&nbsp;</td><td rowspan="3" class="confluenceTd">Resolves error: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.reducer.maxReqsInFlight</td><td class="confluenceTd">1</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.shuffle.io.maxRetries</td><td class="confluenceTd">10</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.scheduler.maxRegisteredResourcesWaitingTime</td><td class="confluenceTd">180s</td><td class="confluenceTd">The maximum amount of time it will wait before scheduling begins is controlled</td><td class="confluenceTd">Resolves error: Application_xxxxx_xxx failed 2 times due to AM container for appattempt_xxxx_xxxxx. Exception from container-launch.</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.dynamicAllocation.enabled</td><td class="confluenceTd">TRUE</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.dynamicAllocation.executorIdleTimeout</td><td class="confluenceTd">60s</td><td class="confluenceTd">Remove executor with if idle for more than this duration</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.dynamicAllocation.cachedExecutorIdleTimeout</td><td class="confluenceTd">36000s</td><td class="confluenceTd">Remove executor with cached data blocks if idle for more than this duration</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.sql.broadcastTimeout</td><td class="confluenceTd">720000</td><td class="confluenceTd">Timeout in seconds for the broadcast wait time in broadcast joins</td><td class="confluenceTd">Resolves error: ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.TimeoutException: Futures timed out after</td><td class="confluenceTd">&nbsp;</td></tr><tr><td rowspan="2" class="confluenceTd">spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version</td><td rowspan="2" class="confluenceTd">2</td><td class="confluenceTd">Major difference between mapreduce.fileoutputcommitter.algorithm.version=1 and 2 is :</td><td rowspan="2" class="confluenceTd">Allows reducers to do mergePaths() to move those files to the final output directory</td><td rowspan="2" class="confluenceTd"><a href="http://www.openkb.info/2019/04/what-is-difference-between.html">http://www.openkb.info/2019/04/what-is-difference-between.html</a></td></tr><tr><td class="confluenceTd">Either AM or Reducers will do the mergePaths().</td></tr><tr><td class="confluenceTd">spark.sql.autoBroadcastJoinThreshold</td><td class="confluenceTd">0</td><td class="confluenceTd">Maximum broadcast table is limited by spark default i.e 8gb</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd"><a href="https://github.com/apache/spark/blob/79c66894296840cc4a5bf6c8718ecfd2b08bcca8/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala#L104">Github source code</a></td></tr><tr><td class="confluenceTd">spark.sql.inMemoryColumnarStorage.compressed</td><td class="confluenceTd">TRUE</td><td rowspan="6" class="confluenceTd">Enables compression. Reduce network IO and memory usage using spark compression default codec lz4</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.rdd.compress</td><td class="confluenceTd">TRUE</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.shuffle.compress</td><td class="confluenceTd">TRUE</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.shuffle.spill.compress</td><td class="confluenceTd">TRUE</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.checkpoint.compress</td><td class="confluenceTd">TRUE</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.broadcast.compress</td><td class="confluenceTd">TRUE</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.storage.level</td><td class="confluenceTd">MEMORY_AND_DISK_SER</td><td class="confluenceTd">Spill partitions that don't fit in executor memory. Uses low space (i.e. memory in RAM or storage in SSD)</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd"><a href="https://stackoverflow.com/questions/30520428/what-is-the-difference-between-memory-only-and-memory-and-disk-caching-level-in">StackOverflow</a></td></tr><tr><td class="confluenceTd">spark.serializer</td><td class="confluenceTd">org.apache.spark.serializer.KryoSerializer</td><td class="confluenceTd">Better than default spark serializer</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.hadoop.s3.multipart.committer.conflict-mode</td><td class="confluenceTd">replace</td><td class="confluenceTd">Setting for new Hadoop parquet magic committer</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.shuffle.consolidateFiles</td><td class="confluenceTd">TRUE</td><td class="confluenceTd">Optimization for custom ShuffleHash join implementation. Note that the MergeSort join is default method which is better for large datasets due to memory limitation</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.reducer.maxSizeInFlight</td><td class="confluenceTd">96</td><td class="confluenceTd">Increase data reducers is requested from “map” task outputs in bigger chunks which would improve performance</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.kryoserializer.buffer.max</td><td class="confluenceTd">1024m</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">Resolves error: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 57197</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.sql.shuffle.partitions</td><td class="confluenceTd">10000</td><td class="confluenceTd">Number of partitions during join operation</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.sql.files.maxPartitionBytes</td><td class="confluenceTd">134217728</td><td class="confluenceTd">Reparation file after reading to 128MB each</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr><tr><td class="confluenceTd">spark.scheduler.listenerbus.eventqueue.capacity</td><td class="confluenceTd">20000</td><td class="confluenceTd">Resolves error: ERROR scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler</td><td class="confluenceTd">&nbsp;</td><td class="confluenceTd">&nbsp;</td></tr></tbody></table><p style="margin-left: 60.0px;"><br></p><h3 style="margin-left: 60.0px;"><span style="letter-spacing: 0.0px;"><strong style="letter-spacing: 0.0px;"><strong>Spark executor memory layout:</strong><br><img class="editor-inline-macro" src="./View Source_files/macro(1)" data-macro-name="anchor" data-macro-id="c16c9b93-168a-4717-954b-0e6c1b022090" data-macro-default-parameter="Spark executor memory layout" data-macro-schema-version="1"><br></strong></span></h3><p style="margin-left: 90.0px;"><strong><img class="confluence-embedded-image" height="250" src="./View Source_files/image2019-12-3_10-56-49.png" data-image-src="/download/attachments/217422325/image2019-12-3_10-56-49.png?version=1&amp;modificationDate=1575350809691&amp;api=v2" data-unresolved-comment-count="0" data-linked-resource-id="217422329" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image2019-12-3_10-56-49.png" data-base-url="https://wiki.advisory.com" data-linked-resource-content-type="image/png" data-linked-resource-container-id="217422325" data-linked-resource-container-version="18" title="Parhate, Devendra &gt; Spark job and cluster tuning &gt; image2019-12-3_10-56-49.png" data-location="Parhate, Devendra &gt; Spark job and cluster tuning &gt; image2019-12-3_10-56-49.png" data-image-height="286" data-image-width="800"></strong></p><table style="margin-left: 90.0px;" class="confluenceTable"><colgroup><col><col></colgroup><tbody style="margin-left: 90.0px;"><tr style="margin-left: 90.0px;"><td style="margin-left: 30.0px;" class="confluenceTd"><p>spark.yarn.executor.MemoryOverhead</p></td><td style="margin-left: 30.0px;" class="confluenceTd"><p>3 * 1024 = 3072</p></td></tr><tr><td class="confluenceTd"><p>spark.executor.memory</p></td><td class="confluenceTd"><p>33 * 1024 = <span style="color: rgb(34,34,34);">33792</span></p></td></tr><tr><td class="confluenceTd"><p>spark.memory.fraction</p></td><td class="confluenceTd"><p>0.8 * 34816 = 27852.8</p></td></tr><tr><td class="confluenceTd"><p>spark.memory.storageFraction</p><p>(cache, broadcast, accumulator)</p></td><td class="confluenceTd"><p>0.4 * 34816 = 13926.4</p></td></tr><tr><td class="confluenceTd"><p>User memory</p></td><td class="confluenceTd"><p>( 1.0 - 0.8 ) * 34816 = 6963.2</p></td></tr><tr><td class="confluenceTd"><p>yarn.nodemanager.resource.memory-mb stays around</p></td><td class="confluenceTd"><p>~40GB</p></td></tr></tbody></table><p style="margin-left: 60.0px;"><br></p><h2><span style="letter-spacing: 0.0px;"><strong style="letter-spacing: 0.0px;"><strong>EMR cluster tuning:</strong></strong></span></h2><table class="relative-table confluenceTable" style="margin-left: 30.0px;width: 78.4421%;"><colgroup><col style="width: 7.97761%;"><col style="width: 28.9013%;"><col style="width: 30.161%;"><col style="width: 32.9601%;"><col></colgroup><tbody style="margin-left: 30.0px;"><tr style="margin-left: 30.0px;"><th style="margin-left: 30.0px;text-align: left;" class="confluenceTh"><p>Classification</p></th><th class="confluenceTh"><p>Configuration Properties</p></th><th colspan="1" class="confluenceTh">Value</th><th colspan="1" class="confluenceTh">Usage</th><th colspan="1" class="confluenceTh">Reference</th></tr><tr style="margin-left: 30.0px;"><td style="margin-left: 30.0px;text-align: left;" class="confluenceTd"><p>core-site</p></td><td style="margin-left: 30.0px;" class="confluenceTd"><p class="auto-cursor-target"><span>fs.s3a.server-side-encryption-algorithm</span></p></td><td colspan="1" class="confluenceTd"><span>AES256</span></td><td colspan="1" class="confluenceTd">Enables S3 AES256 data encryption</td><td colspan="1" class="confluenceTd"><br></td></tr><tr><td style="text-align: left;" rowspan="6" class="confluenceTd"><p>yarn-site</p></td><td class="confluenceTd"><p class="auto-cursor-target"><span>yarn.log-aggregation.retain-seconds</span></p></td><td style="margin-left: 30.0px;" colspan="1" class="confluenceTd">-1</td><td style="margin-left: 30.0px;" colspan="1" class="confluenceTd"><p><br></p></td><td colspan="1" class="confluenceTd"><br></td></tr><tr style="margin-left: 30.0px;"><td colspan="1" class="confluenceTd"><span>yarn.nodemanager.vmem-check-enabled</span></td><td colspan="1" class="confluenceTd"><span>false</span></td><td rowspan="2" class="confluenceTd"><p>To disable hard memory <span>restriction causing </span>OOM (out of memory) JVM error</p><p>(Note: Re-partition data in job based on size)</p></td><td rowspan="2" class="confluenceTd"><br></td></tr><tr><td colspan="1" class="confluenceTd"><span>yarn.nodemanager.pmem-check-enabled</span></td><td colspan="1" class="confluenceTd"><span>false</span></td></tr><tr><td colspan="1" class="confluenceTd"><span>yarn.nm.liveness-monitor.expiry-interval-ms</span></td><td colspan="1" class="confluenceTd"><span>360000</span></td><td colspan="1" class="confluenceTd">Increases time to wait until a node manager is considered dead</td><td colspan="1" class="confluenceTd"><br></td></tr><tr><td colspan="1" class="confluenceTd"><span>yarn.resourcemanager.decommissioning.timeout</span></td><td colspan="1" class="confluenceTd">3600</td><td colspan="1" class="confluenceTd">Increases timeout interval to blacklist node</td><td colspan="1" class="confluenceTd"><br></td></tr><tr><td colspan="1" class="confluenceTd"><span>yarn.log-aggregation-enable</span></td><td colspan="1" class="confluenceTd">true</td><td colspan="1" class="confluenceTd">Aggregates logs at driver node</td><td colspan="1" class="confluenceTd"><br></td></tr><tr><td style="text-align: left;" class="confluenceTd"><p>hdfs-site</p></td><td class="confluenceTd"><p class="auto-cursor-target"><span>dfs.replication</span></p></td><td colspan="1" class="confluenceTd">2</td><td colspan="1" class="confluenceTd">HDFS data replication factor for EMR with <span>auto scaling enabled for </span>core nodes</td><td colspan="1" class="confluenceTd"><br></td></tr><tr><td rowspan="3" class="confluenceTd">capacity-scheduler</td><td colspan="1" class="confluenceTd">yarn.scheduler.capacity.resource-calculator</td><td colspan="1" class="confluenceTd">org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</td><td colspan="1" class="confluenceTd">The default resource calculator i.e org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator uses only memory information for allocating containers and CPU scheduling is not enabled by default</td><td colspan="1" class="confluenceTd"><a href="https://stackoverflow.com/questions/29964792/apache-hadoop-yarn-underutilization-of-cores">Stackoverflow</a></td></tr><tr><td colspan="1" class="confluenceTd">yarn.scheduler.capacity.root.default.maximum-capacity</td><td colspan="1" class="confluenceTd">100</td><td rowspan="2" class="confluenceTd">Uses all resources of dedicated cluster</td><td rowspan="2" class="confluenceTd"><br></td></tr><tr><td colspan="1" class="confluenceTd">yarn.scheduler.capacity.root.default.capacity</td><td colspan="1" class="confluenceTd">100</td></tr><tr><td style="text-align: left;" rowspan="6" class="confluenceTd"><p>emrfs-site</p><br><br><br><br><br></td><td class="confluenceTd"><p class="auto-cursor-target"><span>fs.s3.enableServerSideEncryption</span></p></td><td colspan="1" class="confluenceTd">true</td><td colspan="1" class="confluenceTd"><p>Enables S3 AES256 data encryption</p></td><td colspan="1" class="confluenceTd"><br></td></tr><tr><td colspan="1" class="confluenceTd"><span>fs.s3a.attempts.maximum</span></td><td colspan="1" class="confluenceTd">100</td><td class="confluenceTd"><p>Workaround to resolve S3's storage eventual consistency missing file error due to replication in multiple AZ (availability zone)</p></td><td colspan="1" class="confluenceTd"><br></td></tr><tr><td colspan="1" class="confluenceTd"><span>fs.s3a.committer.magic.enabled</span></td><td colspan="1" class="confluenceTd"><span>true</span></td><td class="confluenceTd"><span><span>Setting for </span><span>new</span><span> Hadoop parquet magic </span></span>committer</td><td class="confluenceTd"><br></td></tr><tr><td colspan="1" class="confluenceTd"><span>fs.s3a.connection.maximum</span></td><td colspan="1" class="confluenceTd"><span>250</span></td><td rowspan="3" class="confluenceTd">Increases S3 IO speed</td><td rowspan="3" class="confluenceTd"><span><a href="https://www.ibm.com/support/knowledgecenter/en/SSCRJT_5.0.4/com.ibm.swg.im.bigsql.doc/doc/bigsql_TuneS3.html">IBM article</a></span></td></tr><tr style="margin-left: 30.0px;"><td style="margin-left: 30.0px;" colspan="1" class="confluenceTd"><span>fs.s3a.threads.core</span></td><td style="margin-left: 30.0px;" colspan="1" class="confluenceTd">250</td></tr><tr style="margin-left: 30.0px;"><td style="margin-left: 30.0px;" colspan="1" class="confluenceTd"><span>fs.s3a.fast.upload</span></td><td style="margin-left: 30.0px;" colspan="1" class="confluenceTd"><span>true</span></td></tr></tbody></table><p><br><br></p><p><br></p>
        <p>&nbsp;</p>
    

</body></html>
